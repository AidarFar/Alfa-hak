{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8117510,"sourceType":"datasetVersion","datasetId":4791490}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Cодержание:\n* [First Bullet Header](#first-bullet)\n* [Second Bullet Header](#second-bullet)","metadata":{}},{"cell_type":"markdown","source":"# Команда: Бета Банк \n\n\n**Цель:** Создать CLTV модель, которая будет выдавать вероятности перехода в каждый из 17 продуктовых кластеров в течение 12 месяцев.\n\nАльфа-Банком предоставлены следующие **данные**, описание из файла **feature_description.xlsx**:\n\n-   **`train_data.pqt`и `test_data.pqt` – данные о клиентах за 3 месяца:**\n   \n    Возможно тут описание длатасета\n    - `st_id` – захэшированное id магазина;\n    - `pr_sku_id` – захэшированное id товара;\n    - `date` – дата;\n    - `pr_sales_type_id` – флаг наличия промо;\n    - `pr_sales_in_units` – число проданных товаров без признака промо;\n    - `pr_promo_sales_in_units` – число проданных товаров с признаком промо;\n    - `pr_sales_in_rub` – продажи без признака промо в РУБ;\n    - `pr_promo_sales_in_rub` – продажи с признаком промо в РУБ;\n\n\n  \nМетрикой качества выступает **ROC-AUC**.\n\nДанные о клиентах и масскированы.","metadata":{}},{"cell_type":"markdown","source":"## Библиотеки","metadata":{}},{"cell_type":"code","source":"# Необходимые библиотеки\nfrom sklearn.ensemble import StackingClassifier, RandomForestClassifier, ExtraTreesClassifier\nimport warnings\nfrom IPython.display import display, HTML\n\n\nimport os\nfrom sklearn.utils import resample\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom catboost import CatBoostClassifier\n# import optuna\nfrom catboost import CatBoostClassifier, Pool\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\nimport numpy as np\nimport time\nimport json\n\nimport pandas as pd\npd.set_option('display.float_format', '{:.4f}'.format)\npd.set_option('display.max_rows', 93)\n\n# Отключить все предупреждения временно\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание \n\n1. Качественно оформите код модели\n2. Доработка решения на платформе будет открыта до 18 апреля 12:00\n3. Обязательно наличие .README;\n4. Код должен быть читабелен и понятен;\n5. Решение должно быть воспроизводимо: эксперты должны иметь возможность протестировать ваше решение на финале.","metadata":{}},{"cell_type":"markdown","source":"## Загрузка и изучение данных","metadata":{}},{"cell_type":"code","source":"def read_df(path: str) -> pd.DataFrame:\n    \"\"\"\n    Функция для чтения DataFrame из Parquet-файла.\n\n    Параметры:\n    path (str): Путь к Parquet-файлу.\n\n    Возвращает:\n    pd.DataFrame: DataFrame, прочитанный из Parquet-файла.\n\n    \"\"\"\n    if os.path.exists(path):\n        df = pd.read_parquet(path)\n        print(f'Успешно: Данные {path} загружены')\n        return df\n    else:\n        print(f'Ошибка: {path} не найден')\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Путь до файла train_df\npath_train_df = \"/kaggle/input/df-restore-cal-avg-start-cluster-3-pqt/train_data.pqt\"\n\n# Путь до файла test_df\npath_test_df = \"/kaggle/input/df-restore-cal-avg-start-cluster-3-pqt/test_data.pqt\"\n\ntrain_df = read_df(path=path_train_df)\ntest_df = read_df(path=path_test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Предобработка данных","metadata":{}},{"cell_type":"markdown","source":"### Объединение датасетов\n","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train_df, test_df], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering\n\nТут кратко описание секции","metadata":{}},{"cell_type":"markdown","source":"###  AVG","metadata":{}},{"cell_type":"markdown","source":"Очень много плохих столбцов sum но  cnt хорошие и можно сгенирировать avg","metadata":{}},{"cell_type":"code","source":"df['avg_a_oper_1m'] = df['sum_a_oper_1m'] / df['cnt_a_oper_1m']\ndf['avg_b_oper_1m'] = df['sum_b_oper_1m'] / df['cnt_b_oper_1m']\ndf['avg_c_oper_1m'] = df['sum_c_oper_1m'] / df['cnt_c_oper_1m']\n\ndf['avg_deb_d_oper_1m'] = df['sum_deb_d_oper_1m'] / df['cnt_deb_d_oper_1m']\ndf['avg_cred_d_oper_1m'] = df['sum_cred_d_oper_1m'] / df['cnt_cred_d_oper_1m']\n\ndf['avg_deb_e_oper_1m'] = df['sum_deb_e_oper_1m'] / df['cnt_deb_e_oper_1m']\ndf['avg_cred_e_oper_1m'] = df['sum_cred_e_oper_1m'] / df['cnt_cred_e_oper_1m']\n\n\ndf['avg_deb_f_oper_1m'] = df['sum_deb_f_oper_1m'] / df['cnt_deb_f_oper_1m']\ndf['avg_cred_f_oper_1m'] = df['sum_cred_f_oper_1m'] / df['cnt_cred_f_oper_1m']\n\ndf['avg_deb_g_oper_1m'] = df['sum_deb_g_oper_1m'] / df['cnt_deb_g_oper_1m']\ndf['avg_cred_g_oper_1m'] = df['sum_cred_g_oper_1m'] / df['cnt_cred_g_oper_1m']\n\ndf['avg_deb_h_oper_1m'] = df['sum_deb_h_oper_1m'] / df['cnt_deb_h_oper_1m']\ndf['avg_cred_h_oper_1m'] = df['sum_cred_h_oper_1m'] / df['cnt_cred_h_oper_1m']\n\n\ndf['avg_a_oper_3m'] = df['sum_a_oper_3m'] / df['cnt_a_oper_3m']\ndf['avg_b_oper_3m'] = df['sum_b_oper_3m'] / df['cnt_b_oper_3m']\ndf['avg_c_oper_3m'] = df['sum_c_oper_3m'] / df['cnt_c_oper_3m']\n\ndf['avg_deb_d_oper_3m'] = df['sum_deb_d_oper_3m'] / df['cnt_deb_d_oper_3m']\ndf['avg_cred_d_oper_3m'] = df['sum_cred_d_oper_3m'] / df['cnt_cred_d_oper_3m']\n\ndf['avg_deb_e_oper_3m'] = df['sum_deb_e_oper_3m'] / df['cnt_deb_e_oper_3m']\ndf['avg_cred_e_oper_3m'] = df['sum_cred_e_oper_3m'] / df['cnt_cred_e_oper_3m']\n\ndf['avg_deb_f_oper_3m'] = df['sum_deb_f_oper_3m'] / df['cnt_deb_f_oper_3m']\ndf['avg_cred_f_oper_3m'] = df['sum_cred_f_oper_3m'] / df['cnt_cred_f_oper_3m']\n\ndf['avg_deb_g_oper_3m'] = df['sum_deb_g_oper_3m'] / df['cnt_deb_g_oper_3m']\ndf['avg_cred_g_oper_3m'] = df['sum_cred_g_oper_3m'] / df['cnt_cred_g_oper_3m']\n\ndf['avg_deb_h_oper_3m'] = df['sum_deb_h_oper_3m'] / df['cnt_deb_h_oper_3m']\ndf['avg_cred_h_oper_3m'] = df['sum_cred_h_oper_3m'] / df['cnt_cred_h_oper_3m']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Удаление плохих столбцов","metadata":{}},{"cell_type":"markdown","source":"Буду пробовать не удалять","metadata":{}},{"cell_type":"code","source":"# columns_to_drop = [\n#     'balance_amt_max',\n#     'balance_amt_min',\n#     'balance_amt_day_avg',\n#     'index_city_code',\n#     'max_founderpres',\n#     'min_founderpres',\n#     'ogrn_exist_months',\n#     'sum_a_oper_1m',\n#     'sum_b_oper_1m',\n#     'sum_c_oper_1m',\n#     'sum_deb_d_oper_1m',\n#     'sum_cred_d_oper_1m',\n#     'sum_deb_e_oper_1m',\n#     'sum_cred_e_oper_1m',\n#     'sum_deb_f_oper_1m',\n#     'sum_cred_f_oper_1m',\n#     'sum_deb_g_oper_1m',\n#     'sum_cred_g_oper_1m',\n#     'sum_deb_h_oper_1m',\n#     'sum_cred_h_oper_1m',\n#     'sum_a_oper_3m',\n#     'sum_b_oper_3m',\n#     'sum_c_oper_3m',\n#     'sum_deb_d_oper_3m',\n#     'sum_cred_d_oper_3m',\n#     'sum_deb_e_oper_3m',\n#     'sum_cred_e_oper_3m',\n#     'sum_deb_f_oper_3m',\n#     'sum_cred_f_oper_3m',\n#     'sum_deb_g_oper_3m',\n#     'sum_cred_g_oper_3m',\n#     'sum_deb_h_oper_3m',\n#     'sum_cred_h_oper_3m']\n\n\n# df = df.drop(columns=columns_to_drop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Восстановление категориальных данных","metadata":{}},{"cell_type":"code","source":"def restore_cal(x):\n    if x.isna().any() and not x.isna().all():\n      return x.fillna(x.dropna().iloc[-1])\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time \n\ncat_columns_to_restore = ['channel_code', 'city',\n                          'city_type', 'ogrn_month', 'ogrn_year', 'okved', 'segment']\n\nfor column in cat_columns_to_restore:\n  df[column] = df.groupby('id')[column].apply(\n      lambda x: restore_cal(x)).reset_index()[column]\n  print(f\"Колонка - {column} - восстановлена\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_parquet(\"df.pqt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/working/df.pqt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet(\"df.pqt\")\ncat_cols = [\n          \"channel_code\", \"city\", \"city_type\",\n          \"okved\", \"segment\", \"start_cluster\", \"ogrn_month\", \"ogrn_year\",\n      ]\n\n\n\ndf['date'] = df['date'].replace({'month_4': 'month_1', 'month_5': 'month_2', 'month_6': 'month_3'})\n\ndf[cat_cols] = df[cat_cols].astype(\"object\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Создание таблицы с 3 месяцами ","metadata":{}},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = [\n    \"channel_code\", \"city\", \"city_type\",\n    \"okved\", \"segment\", \"ogrn_month\", \"ogrn_year\",\n]\n\ncat_cols_month_1 = [f'{col}_month_1' for col in cat_cols]\ncat_cols_month_2 = [f'{col}_month_2' for col in cat_cols]\n\n\n\npivot_df = df.pivot_table(index='id', columns='date', aggfunc='first')\n\npivot_df.columns = [f'{col[0]}_{col[1]}' for col in pivot_df.columns]\n\npivot_df.reset_index(inplace=True)\npivot_df = pivot_df.drop(\n    columns=['end_cluster_month_1', 'end_cluster_month_2'] + cat_cols_month_1 + cat_cols_month_2, axis=0)\n\ncategorical_columns = pivot_df.select_dtypes(include=['object']).columns\npivot_df[categorical_columns] = pivot_df[categorical_columns].fillna(\"missing\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pivot_df[['start_cluster_month_1', 'start_cluster_month_2', 'start_cluster_month_3']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" len(list(pivot_df.select_dtypes(include=['number']).columns[1:]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_cols = pivot_df.select_dtypes(include=['number']).columns[1:]\n\nfor col in numeric_cols:\n    print(col)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_cols = pivot_df.select_dtypes(include=['number']).columns[1:]\n\nfor col in numeric_cols:\n    pivot_df[f'{col}_diff_2_1'] = pivot_df[f'{col}_month_2'] - pivot_df[f'{col}_month_1']\n\nfor col in numeric_cols:\n    pivot_df[f'{col}_diff_3_2'] = pivot_df[f'{col}_month_3'] - pivot_df[f'{col}_month_2']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pivot_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Воостановление start_claster","metadata":{}},{"cell_type":"code","source":"train_data = df[df['start_cluster_month_3'] != 'missing'].drop(\n    ['id',  'end_cluster_month_3'], axis=1)\npredict_data = df[df['start_cluster_month_3'] == 'missing'].drop(\n    ['id', 'end_cluster_month_3'], axis=1)\n\nX = train_data.drop('start_cluster_month_3', axis=1)\ny = train_data['start_cluster_month_3']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Определение желаемого количества экземпляров каждого класса\n# desired_class_count = 750  # Укажите ваше желаемое количество экземпляров\n\n# # Обработка дисбаланса классов\n# balanced_data = pd.DataFrame()\n# for cluster in train_data['start_cluster_month_3'].unique():\n#     cluster_data = train_data[train_data['start_cluster_month_3'] == cluster]\n#     if len(cluster_data) < desired_class_count:\n#         resampled_data = resample(\n#             cluster_data, replace=True, n_samples=desired_class_count, random_state=42)\n#     else:\n#         resampled_data = cluster_data.sample(\n#             n=desired_class_count, replace=False, random_state=42)\n#     balanced_data = pd.concat([balanced_data, resampled_data])\n# display(balanced_data['start_cluster_month_3'].value_counts())\n\n\n\n# X = balanced_data.drop('start_cluster_month_3', axis=1)\n# y = balanced_data['start_cluster_month_3']\n\n# categorical_columns = X.select_dtypes(include=['object']).columns\n# X[categorical_columns] = X[categorical_columns].fillna(\"missing\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost_model_start_cluster = CatBoostClassifier(iterations=1024,\n                           depth=8,\n                           learning_rate=0.075,\n                           random_seed=47,\n                           loss_function='MultiClass',\n                           task_type=\"GPU\",\n                           devices='0',\n                           early_stopping_rounds=20\n                           )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_catboost(model, x_train, y_train, x_val, y_val, cat_names):\n\n    model.fit(\n        x_train, y_train,\n        cat_features=np.array(cat_names),\n        eval_set=(x_val, y_val),\n        verbose=100  # через сколько итераций выводить стату\n    )\n    model.save_model('catboost_model_start_cluster.json')  # сохранение модели\n    feature_importance = model.get_feature_importance(\n        prettified=True)  # датасет с важностью признаков\n\n    return feature_importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_names = X.select_dtypes(include=['object']).columns\n\nfeature_importance = train_catboost(\n    catboost_model_start_cluster, X, y, X_val, y_val, cat_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_predict = predict_data.drop('start_cluster_month_3', axis=1)\npredicted_clusters = catboost_model_start_cluster.predict(X_predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_clusters_flat = np.ravel(predicted_clusters)\nclass_counts = pd.Series(predicted_clusters_flat).value_counts()\nprint(class_counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_index = 0\n\ndf_restore_start_cluster = df.copy()\nfor index, row in df_restore_start_cluster.iterrows():\n    # Проверяем, содержится ли в столбце 'date' значение 'month6' и id >= 100000\n    if row['id'] >= 200000:\n        # Вставляем значение из серии в столбец 'start_cluster_month_3' текущей строки\n        df_restore_start_cluster.at[index,\n                                    'start_cluster_month_3'] = predicted_clusters[predicted_index][0]\n        # Увеличиваем индекс текущей строки в серии\n        predicted_index += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matching_rows = df_restore_start_cluster[df_restore_start_cluster['id'] >= 200000].loc[(df_restore_start_cluster['start_cluster_month_1'] == df_restore_start_cluster['start_cluster_month_2']) & (\n    df_restore_start_cluster['start_cluster_month_2'] == df_restore_start_cluster['start_cluster_month_3'])]\nmatching_rows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_parquet(\"df_0.902.pqt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Обучение модели <a class=\"anchor\" id=\"first-bullet\"></a>","metadata":{}},{"cell_type":"code","source":"train_df = df_restore_start_cluster[df_restore_start_cluster['id']< 200000]\ntest_df = df_restore_start_cluster[df_restore_start_cluster['id'] >= 200000]\n\nX = train_df.drop([\"id\"], axis=1) # оставляю end_cluster чтобы получить пропорцию классов, а потом ниже удалю в коде\ny = train_df[\"end_cluster_month_3\"]\n\nx_train, x_val, y_train, y_val = train_test_split(X, y,\n                                                  test_size=0.2,\n                                                  random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train['end_cluster_month_3'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = x_train['end_cluster_month_3']\nx_train = x_train.drop(['end_cluster_month_3'], axis=1)\nx_val = x_val.drop(['end_cluster_month_3'], axis=1)\n\ndisplay(x_train.shape, y_train.shape, x_val.shape, y_val.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost_model_end_cluster = CatBoostClassifier(iterations=2000,\n                           depth=6,\n                           learning_rate=0.075,\n                           random_seed=47,\n                           loss_function='MultiClass',\n                           task_type=\"GPU\",\n                           devices='0',\n                           early_stopping_rounds=20\n                          )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_catboost(model, x_train, y_train, x_val, y_val, cat_names):\n\n    model.fit(\n    x_train, y_train,\n    cat_features=np.array(cat_names),\n    eval_set=(x_val, y_val),\n    verbose=15 # через сколько итераций выводить стату\n    )\n    model.save_model('catboost_model_end_claster.json') # сохранение модели\n    feature_importance = model.get_feature_importance(prettified=True) # датасет с важностью признаков\n\n    return feature_importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_names = x_train.select_dtypes(include=['object']).columns\n\n\n\nfeature_importance = train_catboost(catboost_model_end_cluster, x_train, y_train, x_val, y_val, cat_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Тестирование модели","metadata":{}},{"cell_type":"code","source":"def weighted_roc_auc(y_true, y_pred, labels, weights_dict):\n    unnorm_weights = np.array([weights_dict[label] for label in labels])\n    weights = unnorm_weights / unnorm_weights.sum()\n    classes_roc_auc = roc_auc_score(y_true, y_pred, labels=labels,\n                                    multi_class=\"ovr\", average=None)\n    return sum(weights * classes_roc_auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_weights = pd.read_excel(\"/kaggle/input/df-restore-cal-avg-start-cluster-3-pqt/cluster_weights.xlsx\").set_index(\"cluster\")\nweights_dict = cluster_weights[\"unnorm_weight\"].to_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_proba = catboost_model_end_cluster.predict_proba(x_val)\nweighted_roc_auc(y_val, y_pred_proba, catboost_model_end_cluster.classes_, weights_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Прогноз на тестовой выборке","metadata":{}},{"cell_type":"code","source":"sample_submission_df = pd.read_csv(\"/kaggle/input/df-restore-cal-avg-start-cluster-3-pqt/sample_submission.csv\") # поменять на свой\nlast_m_test_df = test_df\nlast_m_test_df = last_m_test_df.drop([\"id\" , 'end_cluster_month_3'], axis=1)\n\npool2 = Pool(data=last_m_test_df, cat_features=np.array(cat_names))\n\ntest_pred_proba = catboost_model_end_cluster.predict_proba(pool2) # last_m_test_df\ntest_pred_proba_df = pd.DataFrame(test_pred_proba, columns=catboost_model_end_cluster.classes_)\nsorted_classes = sorted(test_pred_proba_df.columns.to_list())\ntest_pred_proba_df = test_pred_proba_df[sorted_classes]\n\nsample_submission_df[sorted_classes] = test_pred_proba_df\nsample_submission_df.to_csv(\"catboost_1.csv\", index=False) # сохранение модели","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## Выводы и резюме\n\nМы решали **задачу прогнозирования временного ряда спроса товаров** собственного производства на 14 дней вперёд. \n\nЗаказчиком предоставлены исторические данные о **продажах за 1 год**, а также в закодированном виде товарная иерархия и информация о магазинах.  \nПрогнозировалось **число проданных товаров в штуках  `pr_sales_in_units`** для каждого **SKU/товара** (2050 шт. в обучающей выборке) в каждом из **10 магазинов**.\n\nОсновные **закономерности**, выявленные в результате анализа: \n- ***Годовой тренд***  - спад средних продаж в зимний сезон октябрь-март.\n- ***Недельная сезонность*** - пик продаж в субботу, спад в понедельник.\n- В течение года несколько высоких ***пиков спроса, в основном в районе праздников***. Самые резкие подъёмы продаж в период Нового года и Пасхи. Подъем продаж начинается за несколько дней до.\n- 40,6% записей относятся к продажам по промоакциям. Возможны одновременные продажи товара в одном магазине по промо и без. \n- В данных представлены продукты с ***неполными временными рядами***: продавались только в дни около Пасхи, начали продаваться полгода назад.\n- Во всех магазинах разный ассортимент товаров даже при условии одинаковых характеристик торговой точки.\n- Все мета-признаки как характеристики магазинов и товаров показали влияние на средний спрос\n\nНа основе имеющихся данных **сгенерированы новые признаки:**  \n- Календарные: день недели, число месяца, номер недели, флаг выходного дня (взят из доп. таблицы)\n- Лаговые признаки 1-30 дней\n- Скользящее среднее за 7 и 14 предыдущих дней\n- Кластеризация по характеристикам магазинов и товаров\n    \nЧтобы временные ряды каждой комбинации Магазин-Товар были полными создан новый датасет, в который добавлены отсутствующие даты с нулевыми продажами.\n\n Обучение, валидация и выбор лучшего набора гиперпараметров проводится на **кросс-валидации Walk Forward**: подбор гиперпараметров на фолде проводится на valid-выборке, оценка лучшей модели на фолде на test-выборке.   \nВ итоге выбрана одна модель среди лучших на каждом фолде.\n\n Предсказание спроса обученной моделью делается последовательно на каждый следующий день с промежуточным перерасчётом лаговых признаков (учитывается предсказанное значение спроса в предыдущий день).\n\n Для оценки модели использовалась метрика качества  **WAPE**, посчитанная на уровне Магазин-Товар-Дата.  \n \nЛучший результат по качеству и скорости показала модель градиентного бустинга **LightGBM**.  <br>\nПолученный результат: WAPE = **0,47**, превышает baseline (предсказание последним известным значением) с метрикой 69%.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}